<mat-toolbar color="primary" class="second-toolbar">
  <mat-toolbar-row></mat-toolbar-row>
  <mat-toolbar-row>
    <h1>DQN-Method</h1>
  </mat-toolbar-row>
</mat-toolbar>
<mat-card>
  <mat-card-header>
    <mat-card-title>DQN-Method</mat-card-title>
  </mat-card-header>
  <mat-card-content>
    <p>
      The <code>DQNSolver</code> allows to take actions in an environment and adapt it's decision making over time through environmental feedback.
    </p>
    <h4>Class Structure</h4>
    <p>
      The following UML-Class diagram shows the composition of the Solver:
    </p>
    <p>
      <a target="_blank" href="https://github.com/mvrahden/reinforce-js/blob/master/docs/img/dqn-solver-uml.svg">
        <img src="https://rawgit.com/mvrahden/reinforce-js/master/docs/img/dqn-solver-uml.svg"
          width="550px" height="250px" style="max-width:100%;">
      </a>
    </p>
    <p>
      The construction of a solver requires instances of the Classes <code>DQNEnv</code> and <code>DQNOpt</code>.
      These instances provide all necessary information about the environment and the decision-making and learning of the Agent.
    </p>
    <ul>
      <li>
        <strong>DQNEnv</strong> holds information about the environment.
        <ul>
          <li><code>numberOfStates</code> describes the length of the Vector (<strong>s</strong>) of observations</li>
          <li><code>numberOfActions</code> describes the number of potential actions to choose from</li>
        </ul>
      </li>
      <li>
        <strong>DQNOpt</strong> holds all the hyperparamters for the <em>learning</em> and <em>decision making</em>.
        <ul>
          <li><code>numberOfHiddenUnits</code>: depth of the neural network</li>
          <li><code>trainingMode</code>: mode of training, defines the explorative behaviour of the solver (decaying or stable)</li>
          <li><code>epsilon</code>: static exploration rate (during non-training mode)</li>
          <li><code>epsilonMax</code>, <code>epsilonMin</code>, <code>epsilonDecayPeriod</code>: linear decay of exploration during training mode</li>
          <li><code>alpha</code>: discount factor for the network update</li>
          <li><code>gamma</code>: discount factor for the bellmann function</li>
          <li><code>doLossClipping</code>: controll flag for the clipping of the loss function during learning</li>
          <li><code>lossClamp</code>: the size of the loss clipping</li>
          <li><code>doRewardClipping</code>: controll flag for the clipping of the injected reward</li>
          <li><code>rewardClamp</code>: the size of the reward clipping</li>
          <li><code>experienceSize</code>: size of the replay memory</li>
          <li><code>replayInterval</code>: interval of memory replay</li>
          <li><code>replaySteps</code>: size of the replay sample</li>
        </ul>
      </li>
    </ul>
    <h4>State-Action-Reward Scheme</h4>
    <p>
      The solver provides primarily two methods.
      The <code>decide</code>-method allows for the decision making and the <code>learn</code>-method enables the adaptive learning of the solver.
      To decide upon an action the solver needs a vector (<code>s: Array&lt;number&gt;</code>) of current observations (states) an returns the <strong>index</strong> of the action (<code>a: number</code>) to be executed.
      After the action was executed the solver takes an integer reward (<code>r: number</code>) to <code>learn</code> from the effectivity of the action.
    </p>
    <p>
      The appended scheme gives a brief overview of the concept of the solver.
      As seen, the <code>DQNSolver</code> is constructed with a set of hyperparameters defined via <code>DQNOpt</code> and the description of the environment defined by <code>DQNEnv</code>.
      The solver is then enabled to make decisions based on a set of observations (<strong>s</strong>).
      The decision making can then be influenced by providing a reward value.
    </p>
    <p>
      Assuming the solver has decided and learned at least once, the <strong>s</strong>tate and <strong>a</strong>ction of the current iteration plus the <strong>r</strong>eward, <strong>s</strong>tate and <strong>a</strong>ction of the last iteration resolve into an experience (called sarsa-tuple) for the replay memory.
      When aiming for a stable learning, the clipping of the <em>reward</em> and <em>loss</em> is recommended.
    </p>
    <p>
      <a target="_blank" href="https://github.com/mvrahden/reinforce-js/blob/master/docs/img/dqn-solver.svg">
        <img src="https://rawgit.com/mvrahden/reinforce-js/master/docs/img/dqn-solver.svg"
          width="600px" height="420px" style="max-width:100%;">
      </a>
    </p>
    <p>For further information please consult the research paper of
      <a href="http://www.nature.com/articles/nature14236" rel="nofollow">Mnih et al. (2015)
        <em>"Human-level control through deep reinforcement learning"</em>
      </a>
    </p>
  </mat-card-content>
</mat-card>
