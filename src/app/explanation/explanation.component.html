<mat-toolbar color="primary" class="second-toolbar">
  <mat-toolbar-row></mat-toolbar-row>
  <mat-toolbar-row>
    <h1>Explanation</h1>
  </mat-toolbar-row>
</mat-toolbar>
<mat-card>
  <mat-card-header>
    <mat-card-title>Explanation</mat-card-title>
  </mat-card-header>
  <mat-card-content>
    <mat-expansion-panel>
      <mat-expansion-panel-header>
        <mat-panel-title>
          <div style="width: 185px;">World composition</div>
        </mat-panel-title>
        <mat-panel-description style="align-content: left;">
          <div style="width: 300px;">Description of the world components</div>
        </mat-panel-description>
      </mat-expansion-panel-header>
      <p>
        The canvases in the Tabs <b>Pre-Trained</b> and <b>Simulation</b> show a 2-dimensional world with two agents navigating around and roughly 50 items floating randomly around in it.
      </p>
      <p>
        Both agents are composed with different sensory but with an identical dimensioned "brain" (DQN-Solver).
        One of the agents has a <i>ray-based</i> sensory and the other one has a <i>field-based</i> sensory.
        Either sensory enables the agents to detect objects (e.g. items, walls and other agents) in their environment.
        On collision with an item, the agents are either rewarded or punished, depending on their respective valuation.
        Based on these incentivizing feedbacks, the agents ultimate goal is to learn to avoid the harmful items and collect the beneficial ones by accelerating either towards or away from the items accordingly.
        Additionally to the rewards and punishments based on experienced collisions with the items, the agents will experience punishments grounded on their perception of walls and other rivaling agents. 
      </p>
      <p>
        In the <b>Pre-Trained</b>-Section you will find two pre-trained agents from a former simulation run.
        The <b>Simulation</b>-Section shows two freshly initialized agents, that are to be trained during a simulation run.
        As it can be observed, the pre-trained agents are already pretty good at collecting the beneficial items (<font color="green">green</font>) and avoid the harmful ones (<font color="red">red</font>).
      </p>
    </mat-expansion-panel>
    <mat-expansion-panel>
      <mat-expansion-panel-header>
        <mat-panel-title>
          <div style="width: 185px;">Project Structure</div>
        </mat-panel-title>
        <mat-panel-description style="align-content: left;">
          <div style="width: 300px;">Description of the project dependencies</div>
        </mat-panel-description>
      </mat-expansion-panel-header>
      <p>
        The project is structured into several components.
        The frontend view, which you can currently see on this website is located in the <a href="https://github.com/mvrahden/learning-agents/"><b>Learning Agents</b></a> repository.
        This frontend view contains the whole simulation control and the utilities to visualize the logic of the model.
        The model is located in the <a href="https://github.com/mvrahden/learning-agents-model/"><b>Learning Agents Model</b></a> repository.
        It contains the logic for the behaviors and states of each world component, be it an agent, an item or a wall.
        With all the sensory, the inference and the actioning the agents are the world components harboring a gross of the logic.
        The whole logic for the perception, the reward and punishment calculation and the actioning is located within this repository.
      </p>
      <p>
        In contrast the logic for the inference is located in the <a href="https://github.com/mvrahden/reinforce-js/"><b>reinforce-js</b></a> library.
        This inference logic is based on the <b>DQN-Method</b> by <a href="https://www.nature.com/articles/nature14236"><i>Mnih et al. (2015)</i> - 'Human-level control through deep reinforcement learning'</a>, which itself is based on <b>neural networks</b> in combination with <b>reinforcement learning</b>.
        The <b>reinforce-js</b> library itself makes use of the <a href="https://github.com/mvrahden/recurrent-js/"><b>recurrent-js</b></a> library to generate and train the neural networks.
      </p>
    </mat-expansion-panel>
    <mat-expansion-panel>
      <mat-expansion-panel-header>
        <mat-panel-title>
          <div style="width: 185px;">Simulation</div>
        </mat-panel-title>
        <mat-panel-description style="align-content: left;">
          <div style="width: 300px;">Description of the simulation sequence</div>
        </mat-panel-description>
      </mat-expansion-panel-header>
      <p>
        The simulation sequence is configured according to the simulation process of the <a href="https://www.nature.com/articles/nature14236"><i>Mnih et al. (2015)</i> - 'Human-level control through deep reinforcement learning'</a> research paper.
        It is split into two phases a first phase which we will call <b>Trainings Mode</b> and a second phase which we'll call <b>Evaluation Mode</b>.
        The trainings mode is automatically initialized as the simulation panel is opened.
        During this mode the agents inference mechanism is configured to <u>explore</u> their environments state space &ndash; meaning the agents will have a high but over time decaying degree of random navigation behaviors.
        This exploration vs. exploitation decay is linear over time, enforced via the hyperparameter configuration of the individual agents.
      </p>
      <p>
        After 2.5 million iterations (with 24 iterations per second at normal speed and &asymp;216 iterations per second at fast speed) the simulation automatically switches the environment and the agents into evaluation mode.
        During the evaluation mode the agents will have a toned down exploration value, perhaps around 5&#37;.
        This ensures that most of the exposed behavior will be according to what the neural network of each agent will have approximated, hence it will expose what it has learned to predict.
      </p>
    </mat-expansion-panel>
    <mat-expansion-panel>
      <mat-expansion-panel-header>
        <mat-panel-title>
          <div style="width: 185px;">Hyperparameter</div>
        </mat-panel-title>
        <mat-panel-description style="align-content: left;">
          <div style="width: 300px;">Configuration of the hyperparameters</div>
        </mat-panel-description>
      </mat-expansion-panel-header>
      <p>
        Hyperparameters are a set of constants, that are used to configure the initial state of an agents inference mechanism.
        In the following list, we provide an overview of the used hyperparameter configuration during this simulation.
      </p>
      <h4>Parameters Depending on the Environment</h4>
      <div id="data-table">
        <table width="100%" class="highlight">
          <thead>
            <tr>
              <th>Symbol</th>
              <th>Name</th>
              <th>Value</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><i>n<sub>s</sub></i></td>
              <td><code>numberOfStates</code></td>
              <td>152</td>
              <td style="text-align: left;">Perceptable Parameters and size of input layer</td>
            </tr>
            <tr>
              <td><i>n<sub>a</sub></i></td>
              <td><code>numberOfActions</code></td>
              <td>4</td>
              <td style="text-align: left;">Available executable Actions for Agents</td>
            </tr>
          </tbody>
        </table>
      </div>
      <h4>Parameters defining the overall Inference Accuracy</h4>
      <div id="data-table">
        <table width="100%" class="highlight">
          <thead>
            <tr>
              <th>Symbol</th>
              <th>Name</th>
              <th>Value</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><i>n<sub>h</sub></i></td>
              <td><code>numberOfHiddenUnits</code></td>
              <td>100</td>
              <td style="text-align: left;">Dimension of neural network approximator</td>
            </tr>
            <tr>
              <td><i><sub></sub></i></td>
              <td></td>
              <td></td>
              <td style="text-align: left;"></td>
            </tr>
          </tbody>
        </table>
      </div>
      <h4>Parameters defining the Decision Behavior</h4>
      <div id="data-table">
        <table width="100%" class="highlight">
          <thead>
            <tr>
              <th>Symbol</th>
              <th>Name</th>
              <th>Value</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><i>&epsilon;<sub>max</sub></i></td>
              <td><code>epsilonMax</code></td>
              <td>1.00</td>
              <td style="text-align: left;">Upper bound of exploration during training</td>
            </tr>
            <tr>
              <td><i>&epsilon;<sub>min</sub></i></td>
              <td><code>epsilonMax</code></td>
              <td>0.10</td>
              <td style="text-align: left;">Lower bound of exploration during training</td>
            </tr>
            <tr>
              <td><i>t<sub>&epsilon;</sub></i></td>
              <td><code>epsilonDecayPeriod</code></td>
              <td>1e6</td>
              <td style="text-align: left;">Duration of exploration-decay</td>
            </tr>
            <tr>
              <td><i>&epsilon;</i></td>
              <td><code>epsilon</code></td>
              <td>0.05</td>
              <td style="text-align: left;">Static exploration during evaluation mode</td>
            </tr>
          </tbody>
        </table>
      </div>
      <h4>Parameters defining the Learning Behavior</h4>
      <div id="data-table">
        <table width="100%" class="highlight">
          <thead>
            <tr>
              <th>Symbol</th>
              <th>Name</th>
              <th>Value</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><i>&gamma;</i></td>
              <td><code>gamma</code></td>
              <td>0.95</td>
              <td style="text-align: left;">Discount factor of the Bellman equation</td>
            </tr>
            <tr>
              <td><i>&alpha;</i></td>
              <td><code>alpha</code></td>
              <td>0.01</td>
              <td style="text-align: left;">Discount factor for the neural network update</td>
            </tr>
            <tr>
              <td><i><sub></sub></i></td>
              <td><code>doLossClipping</code></td>
              <td><code>true</code></td>
              <td style="text-align: left;">Toggle loss clipping</td>
            </tr>
            <tr>
              <td><i>L<sub>clamp</sub></i></td>
              <td><code>lossClamp</code></td>
              <td>1.00</td>
              <td style="text-align: left;">Clamp for loss value, for learning stabilization</td>
            </tr>
            <tr>
              <td><i><sub></sub></i></td>
              <td><code>doRewardClipping</code></td>
              <td><code>true</code></td>
              <td style="text-align: left;">Toggle reward clipping</td>
            </tr>
            <tr>
              <td><i>r<sub>clamp</sub></i></td>
              <td><code>rewardClamp</code></td>
              <td>1.00</td>
              <td style="text-align: left;">Clamp for reward value, for learning stabilization</td>
            </tr>
            <tr>
              <td><i>n<sub>Exp</sub></i></td>
              <td><code>experienceSize</code></td>
              <td>1e6</td>
              <td style="text-align: left;">Size of the experience memory</td>
            </tr>
            <tr>
              <td></td>
              <td><code>keepExperience-Interval</code></td>
              <td>25</td>
              <td style="text-align: left;">Interval for memorizing experiences</td>
            </tr>
            <tr>
              <td></td>
              <td><code>replaySteps</code></td>
              <td>10</td>
              <td style="text-align: left;">Amount of memory replay steps</td>
            </tr>
          </tbody>
        </table>
      </div>
    </mat-expansion-panel>
  </mat-card-content>
</mat-card>
