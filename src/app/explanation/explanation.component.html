<mat-toolbar color="primary" class="second-toolbar">
  <mat-toolbar-row></mat-toolbar-row>
  <mat-toolbar-row>
    <h1>Explanation</h1>
  </mat-toolbar-row>
</mat-toolbar>
<mat-card>
  <mat-card-header>
    <mat-card-title>Explanation</mat-card-title>
  </mat-card-header>
  <mat-card-content>
    <mat-expansion-panel>
      <mat-expansion-panel-header>
        <mat-panel-title>
          <div style="width: 185px;">World composition</div>
        </mat-panel-title>
        <mat-panel-description style="align-content: left;">
          <div style="width: 300px;">Description of the world components</div>
        </mat-panel-description>
      </mat-expansion-panel-header>
      <p>
        The canvases in the Tabs <b>Pre-Trained</b> and <b>Simulation</b> show a 2-dimensional world with two agents navigating around and roughly 50 items floating randomly around in it.
      </p>
      <p>
        Both agents are composed with different sensory but with an identical dimensioned "brain" (DQN-Solver).
        One of the agents has a <i>ray-based</i> sensory and the other one has a <i>field-based</i> sensory.
        Either sensory enables the agents to detect objects (e.g. items, walls and other agents) in their environment.
        On collision with an item, the agents are either rewarded or punished, depending on their respective valuation.
        Based on these incentivizing feedbacks, the agents ultimate goal is to learn to avoid the harmful items and collect the beneficial ones by accelerating either towards or away from the items accordingly.
        Additionally to the rewards and punishments based on experienced collisions with the items, the agents will experience punishments grounded on their perception of walls and other rivaling agents. 
      </p>
      <p>
        In the <b>Pre-Trained</b>-Section you will find two pre-trained agents from a former simulation run.
        The <b>Simulation</b>-Section shows two freshly initialized agents, that are to be trained during a simulation run.
        As it can be observed, the pre-trained agents are already pretty good in contrast to the freshly initialized agents at collecting the beneficial items (<font color="green">green</font>) and avoiding the harmful ones (<font color="red">red</font>).
        The environment logic ensures that new items are spawned randomly, should an item be consumed by an agent.
        This means, an item has a random type, a random location and a random speed.
      </p>
    </mat-expansion-panel>
    <mat-expansion-panel>
      <mat-expansion-panel-header>
        <mat-panel-title>
          <div style="width: 185px;">Project Structure</div>
        </mat-panel-title>
        <mat-panel-description style="align-content: left;">
          <div style="width: 300px;">Description of the project dependencies</div>
        </mat-panel-description>
      </mat-expansion-panel-header>
      <p>
        The project is structured into several components.
        The frontend view, which you can currently see on this website is located in the <a href="https://github.com/mvrahden/learning-agents/"><b>Learning Agents</b></a> repository.
        This frontend view contains the whole simulation control and the utilities to visualize the logic of the model.
        The model is located in the <a href="https://github.com/mvrahden/learning-agents-model/"><b>Learning Agents Model</b></a> repository.
        It contains the logic for the behaviors and states of each world component, be it an agent, an item or a wall.
        With all the sensory, the inference and the actioning the agents are the world components harboring a gross of the logic.
        The whole logic for the perception, the reward and punishment calculation and the actioning is located within this repository.
      </p>
      <p>
        In contrast the logic for the inference is located in the <a href="https://github.com/mvrahden/reinforce-js/"><b>reinforce-js</b></a> library.
        This inference logic is based on the <b>DQN-Method</b> presented by <a href="https://www.nature.com/articles/nature14236"><i>Mnih et al. (2015)</i></a> in the research paper <i>"Human-level control through deep reinforcement learning"</i>, which itself is based on <b>neural networks</b> in combination with <b>reinforcement learning</b>.
        The <b>reinforce-js</b> library itself makes use of the <a href="https://github.com/mvrahden/recurrent-js/"><b>recurrent-js</b></a> library to generate and train the neural networks.
      </p>
    </mat-expansion-panel>
    <mat-expansion-panel>
      <mat-expansion-panel-header>
        <mat-panel-title>
          <div style="width: 185px;">Simulation</div>
        </mat-panel-title>
        <mat-panel-description style="align-content: left;">
          <div style="width: 300px;">Description of the simulation sequence</div>
        </mat-panel-description>
      </mat-expansion-panel-header>
      <p>
        The simulation sequence is configured according to the simulation process of the <a href="https://www.nature.com/articles/nature14236"><i>Mnih et al. (2015)</i> - "Human-level control through deep reinforcement learning"</a> research paper.
        It is split into two phases a first phase which we will call <b>Trainings Mode</b> and a second phase which we'll call <b>Evaluation Mode</b>.
        The trainings mode is automatically initialized as the simulation panel is opened.
        It is a standardized mode which tries to provide equal training conditions for all simulations.
        In this mode the item population is <u>actively controlled</u> to be somewhat evenly distributed.
        During this mode the agents inference mechanism is configured to <u>explore</u> their environments state space &ndash; meaning the agents will have a high but over time decaying degree of random navigation behaviors.
        This exploration vs. exploitation decay is linear over time, enforced via the hyperparameter configuration of the individual agents.
      </p>
      <p>
        After 2.5 million iterations (with 24 iterations per second at normal speed and &asymp;216 iterations per second at fast speed) the simulation automatically switches the environment and the agents into evaluation mode.
        During the evaluation mode the agents will have a toned down exploration value, perhaps around 5&#37;.
        This ensures that most of the exposed behavior will be according to what the neural network of each agent will have approximated, hence it will expose what it has learned to predict.
      </p>
    </mat-expansion-panel>
    <mat-expansion-panel>
      <mat-expansion-panel-header>
        <mat-panel-title>
          <div style="width: 185px;">Hyperparameter</div>
        </mat-panel-title>
        <mat-panel-description style="align-content: left;">
          <div style="width: 300px;">Configuration of the hyperparameters</div>
        </mat-panel-description>
      </mat-expansion-panel-header>
      <p>
        Hyperparameters are a set of constants, that are used to configure the initial state of an agents inference mechanism.
        In the following list, we provide an overview of the used hyperparameter configuration during this simulation.
      </p>
      <h4>Parameters Depending on the Environment</h4>
      <div id="data-table">
        <table width="100%" class="highlight">
          <thead>
            <tr>
              <th>Symbol</th>
              <th>Name</th>
              <th>Value</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><i>n<sub>s</sub></i></td>
              <td><code>numberOfStates</code></td>
              <td>152</td>
              <td style="text-align: left;">Perceptable Parameters and size of input layer</td>
            </tr>
            <tr>
              <td><i>n<sub>a</sub></i></td>
              <td><code>numberOfActions</code></td>
              <td>4</td>
              <td style="text-align: left;">Available executable Actions for Agents</td>
            </tr>
          </tbody>
        </table>
      </div>
      <h4>Parameters defining the overall Inference Accuracy</h4>
      <div id="data-table">
        <table width="100%" class="highlight">
          <thead>
            <tr>
              <th>Symbol</th>
              <th>Name</th>
              <th>Value</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><i>n<sub>h</sub></i></td>
              <td><code>numberOfHiddenUnits</code></td>
              <td>100</td>
              <td style="text-align: left;">Dimension of neural network approximator</td>
            </tr>
            <tr>
              <td><i><sub></sub></i></td>
              <td></td>
              <td></td>
              <td style="text-align: left;"></td>
            </tr>
          </tbody>
        </table>
      </div>
      <h4>Parameters defining the Decision Behavior</h4>
      <div id="data-table">
        <table width="100%" class="highlight">
          <thead>
            <tr>
              <th>Symbol</th>
              <th>Name</th>
              <th>Value</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><i>&epsilon;<sub>max</sub></i></td>
              <td><code>epsilonMax</code></td>
              <td>1.00</td>
              <td style="text-align: left;">Upper bound of exploration during training</td>
            </tr>
            <tr>
              <td><i>&epsilon;<sub>min</sub></i></td>
              <td><code>epsilonMax</code></td>
              <td>0.10</td>
              <td style="text-align: left;">Lower bound of exploration during training</td>
            </tr>
            <tr>
              <td><i>t<sub>&epsilon;</sub></i></td>
              <td><code>epsilonDecayPeriod</code></td>
              <td>1e6</td>
              <td style="text-align: left;">Duration of exploration-decay</td>
            </tr>
            <tr>
              <td><i>&epsilon;</i></td>
              <td><code>epsilon</code></td>
              <td>0.05</td>
              <td style="text-align: left;">Static exploration during evaluation mode</td>
            </tr>
            <tr>
              <td></td>
              <td><code>trainingMode</code></td>
              <td>true</td>
              <td style="text-align: left;">Toggle training and evaluation mode</td>
            </tr>
          </tbody>
        </table>
      </div>
      <h4>Parameters defining the Learning Behavior</h4>
      <div id="data-table">
        <table width="100%" class="highlight">
          <thead>
            <tr>
              <th>Symbol</th>
              <th>Name</th>
              <th>Value</th>
              <th>Description</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><i>&gamma;</i></td>
              <td><code>gamma</code></td>
              <td>0.95</td>
              <td style="text-align: left;">Discount factor of the Bellman equation</td>
            </tr>
            <tr>
              <td><i>&alpha;</i></td>
              <td><code>alpha</code></td>
              <td>0.01</td>
              <td style="text-align: left;">Discount factor for the neural network update</td>
            </tr>
            <tr>
              <td><i><sub></sub></i></td>
              <td><code>doLossClipping</code></td>
              <td><code>true</code></td>
              <td style="text-align: left;">Toggle loss clipping</td>
            </tr>
            <tr>
              <td><i>L<sub>clamp</sub></i></td>
              <td><code>lossClamp</code></td>
              <td>1.00</td>
              <td style="text-align: left;">Clamp for loss value, for learning stabilization</td>
            </tr>
            <tr>
              <td><i><sub></sub></i></td>
              <td><code>doRewardClipping</code></td>
              <td><code>true</code></td>
              <td style="text-align: left;">Toggle reward clipping</td>
            </tr>
            <tr>
              <td><i>r<sub>clamp</sub></i></td>
              <td><code>rewardClamp</code></td>
              <td>1.00</td>
              <td style="text-align: left;">Clamp for reward value, for learning stabilization</td>
            </tr>
            <tr>
              <td><i>n<sub>Exp</sub></i></td>
              <td><code>experienceSize</code></td>
              <td>1e6</td>
              <td style="text-align: left;">Size of the experience memory</td>
            </tr>
            <tr>
              <td></td>
              <td><code>keepExperience-Interval</code></td>
              <td>25</td>
              <td style="text-align: left;">Interval for memorizing experiences</td>
            </tr>
            <tr>
              <td></td>
              <td><code>replaySteps</code></td>
              <td>10</td>
              <td style="text-align: left;">Amount of memory replay steps</td>
            </tr>
          </tbody>
        </table>
      </div>
    </mat-expansion-panel>
    <mat-expansion-panel>
      <mat-expansion-panel-header>
        <mat-panel-title>
          <div style="width: 185px;">Simulation Results</div>
        </mat-panel-title>
        <mat-panel-description style="align-content: left;">
          <div style="width: 300px;">Summary of Simulation Results</div>
        </mat-panel-description>
      </mat-expansion-panel-header>
      <p>
        The simulation in total endures 10 million iterations.
        During the simulation several performance measures are captured.
        For example as in figure 1. and 2. shown the development of the total consumption of items of both agents.
        On the x-Axis one can find the number of iterations in millions.
        On the y-Axis one can find the number of consumed items in thousands.
      </p>
      <mat-grid-list cols="2" rowHeight="55px">
        <mat-grid-tile colspan="1" rowspan="4" style="align-content:center;">
          <img src="./assets/showcase-eval-01-items.png">
        </mat-grid-tile>
        <mat-grid-tile colspan="1" rowspan="4" style="align-content:center;">
          <img src="./assets/showcase-eval-02-items.png">
        </mat-grid-tile>
        <mat-grid-tile colspan="1" rowspan="1" style="text-align:center;">
          <p>
            Fig. 1.) Item consumption of agent 1 over time
          </p>
        </mat-grid-tile>
        <mat-grid-tile colspan="1" rowspan="1" style="text-align:center;">
          <p>
            Fig. 2.) Item consumption of agent 2 over time
          </p>
        </mat-grid-tile>
      </mat-grid-list>
      <p>
        The colored lines represent the measurements under the environmental boundary condition of an actively <u>balanced</u> item population &ndash; meaning that the population is being consciously controlled to be balanced.
        Whereas the dashed lines are representing the measurements under item <u>scarcity</u> (uncontrolled).
        The higher the green line the better, the lower the red line the better.
        One can observe that the performance of both agents regarding the avoidance of the harmful (<font color="red">red</font>) items is getting even better under the condition of scarcity.
      </p>
      <p>
        Figure 3. and 4. present the measurements of the detection punishments in thousands for each agent.
        It is clearly observable, that in all figures there is a learning curve.
        The lower all lines the better.
      </p>
      <mat-grid-list cols="2" rowHeight="55px">
        <mat-grid-tile colspan="1" rowspan="4" style="align-content:center;">
          <img src="./assets/showcase-eval-01-agents-walls.png">
        </mat-grid-tile>
        <mat-grid-tile colspan="1" rowspan="4" style="align-content:center;">
          <img src="./assets/showcase-eval-02-agents-walls.png">
        </mat-grid-tile>
        <mat-grid-tile colspan="1" rowspan="1" style="text-align:center;">
          <p>
            Fig. 1.) Detection punishments of agent 1 over time
          </p>
        </mat-grid-tile>
        <mat-grid-tile colspan="1" rowspan="1" style="text-align:center;">
          <p>
            Fig. 2.) Detection punishments of agent 2 over time
          </p>
        </mat-grid-tile>
      </mat-grid-list>
      <p>
        Even here it is seen, that the learning under the scarcity condition is always outperforming the learning under controlled item population.
        The explanation for this tendency is simple:
        In the scarcity scenario, the population of the beneficial items instantaneously decreases to a very small amount.
        This leads to an excessive exposure of the agents with harmful items, leading them to learn even better to avoid these kind of items.
        Since now there are few beneficial items on the field and a lot of harmful items the agents are not experiencing a lot of rewards.
        Hence, the agents can now distinguish even better the source of the rewards and punishments, leading them to also learn better to avoid wall detection.
      </p>
    </mat-expansion-panel>
  </mat-card-content>
</mat-card>
